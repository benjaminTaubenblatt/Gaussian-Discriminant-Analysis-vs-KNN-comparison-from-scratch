{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import seed\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    numpy_cov_matrix = np.genfromtxt(\"hwk2_datasets/DS1_Cov.txt\", delimiter=',')[:,:-1]\n",
    "    cov_matrix = pd.DataFrame(numpy_cov_matrix)\n",
    "    negative_mean = np.genfromtxt(\"hwk2_datasets/DS1_m_0.txt\", delimiter=',')[:-1]\n",
    "    positive_mean = np.genfromtxt(\"hwk2_datasets/DS1_m_1.txt\", delimiter=',')[:-1]\n",
    "    return cov_matrix, negative_mean, positive_mean\n",
    "\n",
    "cov_matrix, negative_mean, positive_mean = load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(mean, cov, size):\n",
    "    data = np.random.multivariate_normal(mean, cov, size)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative_data = generate_data(negative_mean, cov_matrix, 2000)\n",
    "positive_data = generate_data(positive_mean, cov_matrix, 2000)\n",
    "\n",
    "negative_examples = pd.DataFrame(negative_data) \n",
    "negative_examples[20] = 'negative'\n",
    "\n",
    "positive_examples = pd.DataFrame(positive_data)\n",
    "positive_examples[20] = 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_valid_test_split(dataset, train_split, valid_split, test_split):\n",
    "    if (train_split + valid_split + test_split) != 1:\n",
    "        raise ValueError(\"invalid size for train_split, valid_split, test_split\")\n",
    "    num_rows = dataset.shape[0]\n",
    "    num_cols = dataset.shape[1]\n",
    "    train = list()\n",
    "    valid = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    \n",
    "    train_size = train_split*num_rows\n",
    "    valid_size = valid_split*num_rows\n",
    "    \n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        train.append(dataset_copy.pop(index))\n",
    "        \n",
    "    while len(valid) < valid_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        valid.append(dataset_copy.pop(index))\n",
    "        \n",
    "    return np.array(train), np.array(valid), np.array(dataset_copy)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(neg_ex, pos_ex):\n",
    "    # train/valid/test split\n",
    "    seed(1)\n",
    "    # negative splits\n",
    "    neg_train, neg_valid, neg_test = train_valid_test_split(neg_ex.values, 0.6, 0.2, 0.2)\n",
    "\n",
    "    # positive splits\n",
    "    pos_train, pos_valid, pos_test = train_valid_test_split(pos_ex.values, 0.6, 0.2, 0.2)\n",
    "\n",
    "    # create datasets\n",
    "    train = np.concatenate((neg_train, pos_train),axis=0)\n",
    "    np.random.shuffle(train) \n",
    "    train = pd.DataFrame(train)\n",
    "    \n",
    "    valid = np.concatenate((neg_valid, pos_valid),axis=0)\n",
    "    np.random.shuffle(valid) \n",
    "    valid = pd.DataFrame(valid)\n",
    "    \n",
    "    test = np.concatenate((neg_test, pos_test),axis=0)\n",
    "    np.random.shuffle(test) \n",
    "    test = pd.DataFrame(test)\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "DS1_train, DS1_valid, DS1_test = create_datasets(negative_examples, positive_examples)\n",
    "\n",
    "DS1_train.to_csv(\"DS1_train.csv\")\n",
    "DS1_valid.to_csv(\"DS1_valid.csv\")\n",
    "DS1_test.to_csv(\"DS1_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data_x_y(data):\n",
    "    num_rows = data.shape[0]\n",
    "    num_cols = data.shape[1]\n",
    "    X = data.iloc[:,:-1]\n",
    "    y = data.iloc[:,-1]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train_alpha = split_data_x_y(DS1_train)\n",
    "y_train = y_train_alpha.replace(['negative', 'positive'], [0, 1])\n",
    "\n",
    "X_valid, y_valid_alpha = split_data_x_y(DS1_valid)\n",
    "y_valid = y_valid_alpha.replace(['negative', 'positive'], [0, 1])\n",
    "\n",
    "X_test, y_test_alpha = split_data_x_y(DS1_test)\n",
    "y_test = y_test_alpha.replace(['negative', 'positive'], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_pi_prior(y):\n",
    "    y_copy = list(y)\n",
    "    num_neg = y_copy.count(0)\n",
    "    return num_neg/len(y_copy)\n",
    "\n",
    "# negative\n",
    "def calculate_mean_0(X, y):\n",
    "    num_rows_x = X.shape[0]\n",
    "    num_cols_x = X.shape[1]\n",
    "    y_copy = list(y)\n",
    "    num_ex = len(y_copy)\n",
    "    \n",
    "    num_neg = y_copy.count(0)\n",
    "    x_given_0 = np.zeros(num_cols_x)\n",
    "    \n",
    "    for i in range(num_ex):\n",
    "        xi = X[i,:]\n",
    "        yi = y[i]\n",
    "        if yi == 0:\n",
    "            x_given_0 = x_given_0 + xi\n",
    "    return (x_given_0/num_neg).astype(np.float32)\n",
    "\n",
    "# positive\n",
    "def calculate_mean_1(X, y):\n",
    "    num_rows_x = X.shape[0]\n",
    "    num_cols_x = X.shape[1]\n",
    "    y_copy = list(y)\n",
    "    num_ex = len(y_copy)\n",
    "    \n",
    "    num_pos = y_copy.count(1)\n",
    "    x_given_1 = np.zeros(num_cols_x)\n",
    "    \n",
    "    for i in range(num_ex):\n",
    "        xi = X[i,:]\n",
    "        yi = y[i]\n",
    "        if yi == 1:\n",
    "            x_given_1 = x_given_1 + xi\n",
    "    return (x_given_1/num_pos).astype(np.float32)\n",
    "\n",
    "def calculate_sigma(X, y, m0, m1):\n",
    "    num_rows_x = X.shape[0]\n",
    "    num_cols_x = X.shape[1]\n",
    "    y_copy = list(y)\n",
    "    num_ex = len(y_copy)\n",
    "    num_0 = y_copy.count(0)\n",
    "    num_1 = y_copy.count(1)\n",
    "    N_0 = num_0/num_ex\n",
    "    N_1 = num_1/num_ex\n",
    "    s_0 = np.zeros((num_cols_x, num_cols_x))\n",
    "    s_1 = np.zeros((num_cols_x, num_cols_x))\n",
    "    for i in range(num_ex):\n",
    "        xi = np.array(X[i,:], dtype = np.float32)\n",
    "        yi = y[i]\n",
    "        if yi == 1:\n",
    "            xm = np.reshape((xi-m0), (-1, 1))\n",
    "            sqr_pos = np.dot(xm,xm.T)\n",
    "            s_1 = s_1 + sqr_pos\n",
    "        else:\n",
    "            xm = np.reshape((xi-m1), (-1, 1))\n",
    "            sqr_neg = np.dot(xm,xm.T)\n",
    "            s_0 = s_0 + sqr_neg\n",
    "    return N_0*((1/num_0)*s_0) + N_1*((1/num_1)*s_1)\n",
    "\n",
    "def gda_parameters_maximum_likelihood(X, y):\n",
    "    pi_prior = calculate_pi_prior(y)\n",
    "    m0 = calculate_mean_0(X, y)\n",
    "    m1 = calculate_mean_1(X, y)\n",
    "    sigma = calculate_sigma(X, y, m0, m1)\n",
    "    return pi_prior, m0, m1, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi_prior, m0, m1, sigma = gda_parameters_maximum_likelihood(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_y(y, pi_prior):\n",
    "    if y == 0:\n",
    "        return pi_prior\n",
    "    else:\n",
    "        return 1-pi_prior\n",
    "\n",
    "def pdf(x, mu, sigma):\n",
    "    xm=np.reshape((x-mu), (-1, 1))\n",
    "    dim = x.shape[0]\n",
    "    pi = np.pi\n",
    "    constant = 1./(((2*pi)**(dim/2))*np.linalg.det(sigma)**-0.5)\n",
    "    return constant*np.exp(-(np.dot(np.dot(xm.T, np.linalg.inv(sigma)), xm))/2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_weights(m0, m1, sigma, cl, pi_pr):\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    if(cl == 0):\n",
    "        mum = np.reshape((m0-m1), (-1, 1))\n",
    "        m0_r = np.reshape(m0, (-1, 1))\n",
    "        m1_r = np.reshape(m1, (-1, 1))\n",
    "        w = np.dot(sigma_inv, mum)\n",
    "        w0 = -0.5*np.dot(np.dot(m0_r.T, sigma_inv), m0_r) + 0.5*np.dot(np.dot(m1_r.T, sigma_inv), m1_r) + np.log(prob_y(0, pi_pr)/prob_y(1, pi_pr))\n",
    "        return w, w0\n",
    "    else:\n",
    "        mum = np.reshape((m1-m0), (-1, 1))\n",
    "        m0_r = np.reshape(m0, (-1, 1))\n",
    "        m1_r = np.reshape(m1, (-1, 1))\n",
    "        w = np.dot(sigma_inv, mum)\n",
    "        w0 = -0.5*np.dot(np.dot(m1_r.T, sigma_inv), m1_r) + 0.5*np.dot(np.dot(m0_r.T, sigma_inv), m0_r) + np.log(prob_y(1, pi_pr)/prob_y(0, pi_pr))\n",
    "        return w, w0\n",
    "        \n",
    "\n",
    "# p(Ck|x)\n",
    "def predict(xi, m0, m1, sigma, w_0, w0_0, w_1, w0_1):\n",
    "    aa0 = np.dot(w_0.T, xi) + w0_0\n",
    "    aa1 = np.dot(w_1.T, xi) + w0_1\n",
    "    sig0 = 1 / (1 + np.exp(-aa0))\n",
    "    sig1 = 1 / (1 + np.exp(-aa1))\n",
    "    if(sig0 > sig1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def classify(X, m0, m1, sigma, pi_pr):\n",
    "    num_rows_x = X.shape[0]\n",
    "    predictions = np.zeros(num_rows_x)\n",
    "    w_0, w0_0 = find_weights(m0,m1,sigma, 0, pi_pr)\n",
    "    w_1, w0_1 = find_weights(m0,m1,sigma, 1, pi_pr)\n",
    "    \n",
    "    print(\"Class 1:\")\n",
    "    print(w0_1)\n",
    "    print()\n",
    "    print(w_1)\n",
    "    print()\n",
    "    \n",
    "    print(\"Class 0:\")\n",
    "    print(w0_0)\n",
    "    print()\n",
    "    print(w_0)\n",
    "    print()\n",
    "    \n",
    "    for i in range(num_rows_x):\n",
    "        xi = np.array(X[i,:], dtype=np.float32)\n",
    "        yhat = predict(xi, m0, m1, sigma, w_0, w0_0, w_1, w0_1)\n",
    "        predictions[i] = yhat\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "[[-2.16762388]]\n",
      "\n",
      "[[-1.1498975 ]\n",
      " [ 0.67030086]\n",
      " [ 0.44289768]\n",
      " [ 0.2360709 ]\n",
      " [ 0.77487325]\n",
      " [ 0.34207348]\n",
      " [-1.3237704 ]\n",
      " [ 1.91948175]\n",
      " [ 2.29204001]\n",
      " [-0.7314653 ]\n",
      " [ 1.04502041]\n",
      " [ 0.95466482]\n",
      " [-1.22530663]\n",
      " [-1.03988365]\n",
      " [ 0.44911846]\n",
      " [-1.04061717]\n",
      " [-2.31835761]\n",
      " [ 0.55848259]\n",
      " [ 0.04508336]\n",
      " [ 0.40793269]]\n",
      "\n",
      "Class 0:\n",
      "[[2.16762388]]\n",
      "\n",
      "[[ 1.1498975 ]\n",
      " [-0.67030086]\n",
      " [-0.44289768]\n",
      " [-0.2360709 ]\n",
      " [-0.77487325]\n",
      " [-0.34207348]\n",
      " [ 1.3237704 ]\n",
      " [-1.91948175]\n",
      " [-2.29204001]\n",
      " [ 0.7314653 ]\n",
      " [-1.04502041]\n",
      " [-0.95466482]\n",
      " [ 1.22530663]\n",
      " [ 1.03988365]\n",
      " [-0.44911846]\n",
      " [ 1.04061717]\n",
      " [ 2.31835761]\n",
      " [-0.55848259]\n",
      " [-0.04508336]\n",
      " [-0.40793269]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classify(X_test.values, m0, m1, sigma, pi_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, actual):\n",
    "    num_rows =  predictions.shape[0]\n",
    "    num_correct = 0\n",
    "    for i in range(num_rows):\n",
    "        if(predictions[i] == actual[i]):\n",
    "            num_correct = num_correct + 1\n",
    "    return num_correct/num_rows\n",
    "\n",
    "def get_confusion_matrix(pred, actual):\n",
    "    y_actu = pd.Series(actual, name='Actual')\n",
    "    y_pred = pd.Series(pred, name='Predicted')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "    return df_confusion\n",
    "\n",
    "def get_precision(conf_matrix):\n",
    "    return conf_matrix[0,0]/(conf_matrix[0,0]+conf_matrix[0,1])\n",
    "\n",
    "def get_recall(conf_matrix):\n",
    "    return conf_matrix[0,0]/(conf_matrix[0,0]+conf_matrix[1,0])\n",
    "\n",
    "def get_f_measure(p, r):\n",
    "    return (2*p*r)/(p+r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96, 0.9675, 0.9532019704433498, 0.9602977667493796)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = get_confusion_matrix(predictions, y_test.values)\n",
    "\n",
    "accuracy = get_accuracy(predictions, y_test.values)\n",
    "precision = get_precision(confusion_matrix.values)\n",
    "recall = get_recall(confusion_matrix.values)\n",
    "f_measure = get_f_measure(precision, recall)\n",
    "\n",
    "accuracy, precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def euclideanDistance(test_data, data, length):\n",
    "    dist = 0\n",
    "    for i in range(length):\n",
    "        dist = dist + np.square(test_data[i] - data[i])\n",
    "    return np.sqrt(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn(test_instance, train, k):\n",
    "    num_rows_train = train.shape[0]\n",
    "    num_cols_test = train.shape[1]-1\n",
    "    distances = {}\n",
    "    for i in range(num_rows_train):\n",
    "        dist = euclideanDistance(test_instance, train[i,:-1], num_cols_test)\n",
    "        distances[i] = dist\n",
    "    \n",
    "    sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(sorted_distances[i][0])\n",
    "    \n",
    "    votes = {}\n",
    "    for i in range(len(neighbors)):\n",
    "        response = train[neighbors[i],-1]\n",
    "        if response in votes:\n",
    "            votes[response] += 1\n",
    "        else:\n",
    "            votes[response] = 1\n",
    "            \n",
    "    sorted_votes = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_votes[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_knn(test, train, k):\n",
    "    num_rows_test = test.shape[0]\n",
    "    predictions = []\n",
    "    for i in range(num_rows_test):\n",
    "        result = knn(test[i], train, k)\n",
    "        predictions.append(result)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(param):\n",
    "    num_rows = param.shape[0]\n",
    "    num_cols = param.shape[1]-1\n",
    "    normalized_params = np.zeros((num_rows, num_cols))\n",
    "    final_param = param.copy()\n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        normalized_col = (param[:,i] - np.mean(param[:,i])) / np.std(param[:,i])\n",
    "        normalized_params[:,i] = normalized_col\n",
    "    final_param[:,:-1] = normalized_params\n",
    "    return pd.DataFrame(final_param)\n",
    "\n",
    "def convert_preds_to_df_num(pred):\n",
    "    df_predictions_knn = pd.DataFrame(pred)\n",
    "    df_predictions_knn_num = df_predictions_knn.replace(['negative', 'positive'], [0, 1])\n",
    "    predictions_knn = df_predictions_knn_num.iloc[:,-1]\n",
    "    return predictions_knn\n",
    "\n",
    "def knn_neighbor_comparison(arr, nml_test, nml_train):\n",
    "    performance = {}\n",
    "    for k in arr:\n",
    "        X_te, y_te_alpha = split_data_x_y(nml_test)\n",
    "        y_te = y_te_alpha.replace(['negative', 'positive'], [0, 1])\n",
    "        \n",
    "        X_tr = nml_train\n",
    "        \n",
    "        prediction_knn = classify_knn(X_te.values, X_tr.values, k)\n",
    "        predictions_knn_num = convert_preds_to_df_num(prediction_knn)\n",
    "        \n",
    "        conf_matrix = get_confusion_matrix(predictions_knn_num.values, y_te.values)\n",
    "        accuracy = get_accuracy(predictions_knn_num, y_te.values)\n",
    "        precision = get_precision(conf_matrix.values)\n",
    "        recall = get_recall(conf_matrix.values)\n",
    "        f_measure = get_f_measure(precision, recall)\n",
    "        performance[k] = [accuracy, precision, recall, f_measure]\n",
    "        \n",
    "    return performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [0.52375, 0.5375, 0.5231143552311436, 0.530209617755857],\n",
       " 3: [0.55625, 0.54, 0.5581395348837209, 0.5489199491740787],\n",
       " 5: [0.54625, 0.52, 0.5488126649076517, 0.5340179717586649],\n",
       " 25: [0.5375, 0.545, 0.5369458128078818, 0.5409429280397022],\n",
       " 75: [0.52625, 0.555, 0.524822695035461, 0.5394896719319562],\n",
       " 125: [0.555, 0.5875, 0.5516431924882629, 0.5690072639225181],\n",
       " 225: [0.56125, 0.615, 0.5553047404063205, 0.5836298932384341]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ks = [1,3,5,25,75,125,225]\n",
    "normalized_valid = normalize(DS1_valid.values)\n",
    "normalized_train = normalize(DS1_train.values)\n",
    "\n",
    "performance = knn_neighbor_comparison(all_ks, normalized_valid, normalized_train)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{25: [0.5525, 0.5425, 0.5535714285714286, 0.5479797979797979]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_test = normalize(DS1_test.values)\n",
    "top_performance = knn_neighbor_comparison([25], normalized_test, normalized_train)\n",
    "top_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_ds2_cov():\n",
    "    numpy_cov_1 = np.genfromtxt(\"hwk2_datasets/DS2_Cov1.txt\", delimiter=',')[:,:-1]\n",
    "    numpy_cov_2 = np.genfromtxt(\"hwk2_datasets/DS2_Cov2.txt\", delimiter=',')[:,:-1]\n",
    "    numpy_cov_3 = np.genfromtxt(\"hwk2_datasets/DS2_Cov3.txt\", delimiter=',')[:,:-1]\n",
    "    \n",
    "    cov_matrix_1 = pd.DataFrame(numpy_cov_1)\n",
    "    cov_matrix_2 = pd.DataFrame(numpy_cov_2)\n",
    "    cov_matrix_3 = pd.DataFrame(numpy_cov_3)\n",
    "    \n",
    "    return cov_matrix_1, cov_matrix_2, cov_matrix_3\n",
    "\n",
    "def load_data_ds2_neg_mean():\n",
    "    \n",
    "    negative_mean_1 = np.genfromtxt(\"hwk2_datasets/DS2_c2_m1.txt\", delimiter=',')[:-1]\n",
    "    negative_mean_2 = np.genfromtxt(\"hwk2_datasets/DS2_c2_m2.txt\", delimiter=',')[:-1]\n",
    "    negative_mean_3 = np.genfromtxt(\"hwk2_datasets/DS2_c2_m3.txt\", delimiter=',')[:-1]\n",
    "    \n",
    "    return negative_mean_1, negative_mean_2, negative_mean_3\n",
    "\n",
    "def load_data_ds2_pos_mean():\n",
    "\n",
    "    positive_mean_1 = np.genfromtxt(\"hwk2_datasets/DS2_c1_m1.txt\", delimiter=',')[:-1]\n",
    "    positive_mean_2 = np.genfromtxt(\"hwk2_datasets/DS2_c1_m2.txt\", delimiter=',')[:-1]\n",
    "    positive_mean_3 = np.genfromtxt(\"hwk2_datasets/DS2_c1_m3.txt\", delimiter=',')[:-1]\n",
    "    \n",
    "    return positive_mean_1, positive_mean_2, positive_mean_3\n",
    "    \n",
    "\n",
    "cov_ds2_1, cov_ds2_2, cov_ds2_3 = load_data_ds2_cov()\n",
    "\n",
    "neg_ds2_mean_1, neg_ds2_mean_2, neg_ds2_mean_3 = load_data_ds2_neg_mean()\n",
    "\n",
    "pos_ds2_mean_1, pos_ds2_mean_2, pos_ds2_mean_3 = load_data_ds2_pos_mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data_multi_distr(params, coefficients, sample_size):\n",
    "    if(np.sum(coefficients) != 1):\n",
    "        raise ValueError(\"coefficients do not add to 1!\")\n",
    "    \n",
    "    data = np.zeros((sample_size, 20))\n",
    "    for i in range(sample_size):\n",
    "        distr_index = np.random.choice(3, p = coefficients)\n",
    "        distribution = params[distr_index]\n",
    "        mean = distribution[\"mean\"]\n",
    "        cov_mat = distribution[\"cov\"]\n",
    "        sample_point = generate_data(mean, cov_mat, 1)\n",
    "        data[i] = sample_point\n",
    "    return data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_params = [\n",
    "    {\"mean\": pos_ds2_mean_1, \"cov\": cov_ds2_1},\n",
    "    {\"mean\": pos_ds2_mean_2, \"cov\": cov_ds2_2},\n",
    "    {\"mean\": pos_ds2_mean_3, \"cov\": cov_ds2_3},\n",
    "]\n",
    "\n",
    "negative_params = [\n",
    "    {\"mean\": neg_ds2_mean_1, \"cov\": cov_ds2_1},\n",
    "    {\"mean\": neg_ds2_mean_2, \"cov\": cov_ds2_2},\n",
    "    {\"mean\": neg_ds2_mean_3, \"cov\": cov_ds2_3},\n",
    "]\n",
    "\n",
    "coefficients = np.array([0.1, 0.42, 0.48])\n",
    "\n",
    "positive_data_ds2 = generate_data_multi_distr(positive_params, coefficients, 2000)\n",
    "positive_examples_ds2 = pd.DataFrame(positive_data_ds2)\n",
    "positive_examples_ds2[20] = 'positive'\n",
    "\n",
    "negative_data_ds2 = generate_data_multi_distr(negative_params, coefficients, 2000)\n",
    "negative_examples_ds2 = pd.DataFrame(negative_data_ds2)\n",
    "negative_examples_ds2[20] = 'negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datasets_ds2(neg_ex, pos_ex):\n",
    "    # train/valid/test split\n",
    "    seed(42)\n",
    "    # negative splits\n",
    "    neg_train, neg_valid, neg_test = train_valid_test_split(neg_ex.values, 0.6, 0.2, 0.2)\n",
    "\n",
    "    # positive splits\n",
    "    pos_train, pos_valid, pos_test = train_valid_test_split(pos_ex.values, 0.6, 0.2, 0.2)\n",
    "\n",
    "    # create datasets\n",
    "    train = np.concatenate((neg_train, pos_train),axis=0)\n",
    "    np.random.shuffle(train) \n",
    "    train = pd.DataFrame(train)\n",
    "    \n",
    "    valid = np.concatenate((neg_valid, pos_valid),axis=0)\n",
    "    np.random.shuffle(valid) \n",
    "    valid = pd.DataFrame(valid)\n",
    "    \n",
    "    test = np.concatenate((neg_test, pos_test),axis=0)\n",
    "    np.random.shuffle(test) \n",
    "    test = pd.DataFrame(test)\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DS2_train, DS2_valid, DS2_test = create_datasets_ds2(negative_examples_ds2, positive_examples_ds2)\n",
    "\n",
    "DS2_train.to_csv(\"DS2_train.csv\")\n",
    "DS2_valid.to_csv(\"DS2_valid.csv\")\n",
    "DS2_test.to_csv(\"DS2_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2_train, y2_train_alpha = split_data_x_y(DS2_train)\n",
    "y2_train = y2_train_alpha.replace(['negative', 'positive'], [0, 1])\n",
    "\n",
    "X2_valid, y2_valid_alpha = split_data_x_y(DS2_valid)\n",
    "y2_valid = y2_valid_alpha.replace(['negative', 'positive'], [0, 1])\n",
    "\n",
    "X2_test, y2_test_alpha = split_data_x_y(DS2_test)\n",
    "y2_test = y2_test_alpha.replace(['negative', 'positive'], [0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi_prior_2, m0_2, m1_2, sigma_2 = gda_parameters_maximum_likelihood(X2_train.values, y2_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "[[0.04538512]]\n",
      "\n",
      "[[ 0.00398763]\n",
      " [-0.05683509]\n",
      " [-0.07155181]\n",
      " [-0.01016242]\n",
      " [ 0.04295847]\n",
      " [-0.0485771 ]\n",
      " [-0.01398687]\n",
      " [ 0.003473  ]\n",
      " [ 0.03296811]\n",
      " [ 0.0215139 ]\n",
      " [-0.06979787]\n",
      " [ 0.04639429]\n",
      " [ 0.05374306]\n",
      " [ 0.03564288]\n",
      " [ 0.0232841 ]\n",
      " [-0.02778823]\n",
      " [-0.03966765]\n",
      " [-0.03637554]\n",
      " [ 0.02401795]\n",
      " [ 0.04673235]]\n",
      "\n",
      "Class 0:\n",
      "[[-0.04538512]]\n",
      "\n",
      "[[-0.00398763]\n",
      " [ 0.05683509]\n",
      " [ 0.07155181]\n",
      " [ 0.01016242]\n",
      " [-0.04295847]\n",
      " [ 0.0485771 ]\n",
      " [ 0.01398687]\n",
      " [-0.003473  ]\n",
      " [-0.03296811]\n",
      " [-0.0215139 ]\n",
      " [ 0.06979787]\n",
      " [-0.04639429]\n",
      " [-0.05374306]\n",
      " [-0.03564288]\n",
      " [-0.0232841 ]\n",
      " [ 0.02778823]\n",
      " [ 0.03966765]\n",
      " [ 0.03637554]\n",
      " [-0.02401795]\n",
      " [-0.04673235]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_2 = classify(X2_test.values, m0_2, m1_2, sigma_2, pi_prior_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.52625, 0.495, 0.528, 0.5109677419354839)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_2 = get_confusion_matrix(predictions_2, y2_test.values)\n",
    "\n",
    "accuracy_2 = get_accuracy(predictions_2, y2_test.values)\n",
    "precision_2 = get_precision(confusion_matrix_2.values)\n",
    "recall_2 = get_recall(confusion_matrix_2.values)\n",
    "f_measure_2 = get_f_measure(precision_2, recall_2)\n",
    "\n",
    "accuracy_2, precision_2, recall_2, f_measure_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [0.48875, 0.45, 0.4878048780487805, 0.46814044213263983],\n",
       " 3: [0.53125, 0.5275, 0.5314861460957179, 0.5294855708908407],\n",
       " 5: [0.51125, 0.5, 0.5115089514066496, 0.5056890012642226],\n",
       " 25: [0.5075, 0.4725, 0.5080645161290323, 0.4896373056994819],\n",
       " 75: [0.49, 0.4075, 0.4880239520958084, 0.444141689373297],\n",
       " 125: [0.50875, 0.41, 0.5109034267912772, 0.45492371705963935],\n",
       " 175: [0.51, 0.3725, 0.5137931034482759, 0.4318840579710145],\n",
       " 225: [0.5025, 0.365, 0.503448275862069, 0.4231884057971015]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ks_2 = [1,3,5,25,75,125,175,225]\n",
    "normalized_valid_2 = normalize(DS2_valid.values)\n",
    "normalized_train_2 = normalize(DS2_train.values)\n",
    "\n",
    "performance_2 = knn_neighbor_comparison(all_ks_2, normalized_valid_2, normalized_train_2)\n",
    "performance_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{25: [0.5475, 0.515, 0.5508021390374331, 0.5322997416020671]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_test_2 = normalize(DS2_test.values)\n",
    "top_performance_2 = knn_neighbor_comparison([25], normalized_test_2, normalized_train_2)\n",
    "top_performance_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
